# DYNAMO Configuration - Optimized for T4 GPU Performance
device: cuda
seed: 42
use_wandb: false

# Model Configuration
model:
  base_model_name: roberta-base
  freeze_backbone: true
  hidden_size: 768
  num_tasks: 5
  router_hidden_sizes: [512, 256]
  router_dropout: 0.1
  temperature_init: 1.0
  temperature_learnable: true
  lora_configs:
    sentiment:
      rank: 16
      alpha: 32
      dropout: 0.1
      target_modules: ['query', 'value', 'key', 'dense']
    qa:
      rank: 24
      alpha: 48
      dropout: 0.1
      target_modules: ['query', 'value', 'key', 'dense']
    summarization:
      rank: 32
      alpha: 64
      dropout: 0.1
      target_modules: ['query', 'value', 'key', 'dense']
    code_generation:
      rank: 32
      alpha: 64
      dropout: 0.1
      target_modules: ['query', 'value', 'key', 'dense']
    translation:
      rank: 32
      alpha: 64
      dropout: 0.1
      target_modules: ['query', 'value', 'key', 'dense']

# Training Configuration - Optimized for T4 (16GB memory)
training:
  # Phase-specific epochs (ideal values for DYNAMO training)
  phase1_epochs: 3    # LoRA adapter training - moderate epochs for good task-specific learning
  phase2_epochs: 5    # Router training - more epochs needed for complex routing decisions
  phase3_epochs: 2    # Joint fine-tuning - fewer epochs to avoid overfitting
  
  # Legacy support (fallback if phase-specific not used)
  num_epochs: 3
  
  batch_size: 32  # Optimized for T4 memory + mixed precision
  gradient_accumulation_steps: 2  # Effective batch size: 64
  max_length: 512  # Full sequence length
  warmup_ratio: 0.1
  weight_decay: 0.01
  lora_lr: 5.0e-4
  router_lr: 0.001
  joint_lr: 0.0001
  load_balance_weight: 0.1
  efficiency_weight: 0.05
  consistency_weight: 0.1
  gumbel_temperature: 1.0
  temperature_decay: 0.999
  min_temperature: 0.1
  
  # GPU-specific optimizations
  use_mixed_precision: true
  dataloader_num_workers: 8  # Optimal for T4
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true

# Data Configuration - Optimized for speed
data:
  max_input_length: 512
  max_target_length: 128
  data_dir: ./data
  cache_dir: ./cache
  
  # Dataset sizes (reduced for faster training on T4)
  sst2_size: 8000
  squad_size: 15000
  xsum_size: 10000
  code_gen_size: 6000
  translation_size: 8000
  mixed_task_size: 4000
  
# Evaluation Configuration
evaluation:
  eval_batch_size: 32  # Reduced for memory efficiency
  eval_steps: 500
  save_steps: 1000
  logging_steps: 25  # More frequent logging for monitoring
  visualize_routing: true

# Output directories
output_dir: ./outputs
log_dir: ./logs
checkpoint_dir: ./checkpoints

# DYNAMO Configuration - Optimized for T4 GPU Performance
device: cuda
seed: 42
use_wandb: false

# Model Configuration
model:
  base_model_name: roberta-base
  freeze_backbone: true
  hidden_size: 768
  num_tasks: 5
  router_hidden_sizes: [512, 256]
  router_dropout: 0.1
  temperature_init: 1.0
  temperature_learnable: true
  lora_configs:
    sentiment:
      rank: 16
      alpha: 32
      dropout: 0.1
      target_modules: ['query', 'value', 'key', 'dense']
    qa:
      rank: 24
      alpha: 48
      dropout: 0.1
      target_modules: ['query', 'value', 'key', 'dense']
    summarization:
      rank: 32
      alpha: 64
      dropout: 0.1
      target_modules: ['query', 'value', 'key', 'dense']
    code_generation:
      rank: 32
      alpha: 64
      dropout: 0.1
      target_modules: ['query', 'value', 'key', 'dense']
    translation:
      rank: 32
      alpha: 64
      dropout: 0.1
      target_modules: ['query', 'value', 'key', 'dense']

# Training Configuration - Optimized for T4 (16GB memory)
training:
  # Phase-specific epochs (ideal values for DYNAMO training)
  phase1_epochs: 3    # LoRA adapter training - moderate epochs for good task-specific learning
  phase2_epochs: 5    # Router training - more epochs needed for complex routing decisions
  phase3_epochs: 3    # Joint fine-tuning - better curriculum progression (epochs 0,1,2)
  
  # Legacy support (fallback if phase-specific not used)
  num_epochs: 3
  
  batch_size: 8  # Reduced from 32 to prevent OOM on T4
  gradient_accumulation_steps: 8  # Increased to maintain effective batch size: 64
  max_length: 256  # Reduced from 512 to save memory
  warmup_ratio: 0.1
  weight_decay: 0.01
  lora_lr: 5.0e-4
  router_lr: 0.001
  joint_lr: 0.0001
  load_balance_weight: 0.1
  efficiency_weight: 0.05
  consistency_weight: 0.1
  gumbel_temperature: 1.0
  temperature_decay: 0.999
  min_temperature: 0.1
  
  # Curriculum learning parameters for Phase 2
  curriculum_start_ratio: 0.8
  curriculum_end_ratio: 0.2
  patience: 5  # Early stopping patience
  
  # GPU-specific optimizations
  use_mixed_precision: true
  dataloader_num_workers: 1  # Reduced from 8 to save CPU memory
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: false  # Disabled to save memory

# Data Configuration - Optimized for speed
data:
  max_input_length: 256  # Reduced from 512
  max_target_length: 64   # Reduced from 128
  data_dir: ./data
  cache_dir: ./cache
  
  # Dataset sizes (reduced for faster training on T4)
  sst2_size: 8000
  squad_size: 15000
  xsum_size: 10000
  code_gen_size: 6000
  translation_size: 8000
  mixed_task_size: 4000
  
# Evaluation Configuration
evaluation:
  eval_batch_size: 4  # Further reduced for memory efficiency
  eval_steps: 500
  save_steps: 1000
  logging_steps: 25  # More frequent logging for monitoring
  visualize_routing: true

# Output directories
output_dir: ./outputs
log_dir: ./logs
checkpoint_dir: ./checkpoints
